{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chalwemwansa/data_mining_assign/blob/master/Group_22_data_mining_assign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IerYbU6h9pBb"
      },
      "source": [
        "# 1. Business Understanding\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "The reliability and usefulness of Zambian Wikipedia pages as a source of current information can be affected when the content becomes outdated  \n",
        "In many cases information that once was accurate is left unchanged for long periods of time and this makes it hard for readers to trust that what they are reading reflects the present situation  \n",
        "\n",
        "Without a clear and organized way to identify which pages are stale or contain old data it is difficult for contributors especially Zambian Wikipedians to know where their attention is most needed  \n",
        "This lack of direction means that updates can be random or uneven and some topics remain untouched while others are repeatedly edited  \n",
        "\n",
        "Over time this results in a reduced quality of content on and about Zambia making the platform less valuable as a resource for people who rely on it for research learning or keeping up with developments in the country."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRDGW3GwtZaO"
      },
      "source": [
        "\n",
        "## Business Objectives\n",
        "\n",
        "From a practical real world view success for this project means being able to identify what is old and what is still current  \n",
        "It is about detecting and flagging sections or whole Zambian Wikipedia pages like the Zambia page that hold stale facts  \n",
        "Clear signals help both readers and editors see where drift has happened and how serious it is  \n",
        "\n",
        "The next step is to make updates easier not harder  \n",
        "Contributors especially Zambian Wikipedians need a simple way to spot which pages or sections most need attention for factual recency  \n",
        "This supports the DataLab Research group as it explores strategies for improving contributions on and about Zambia  \n",
        "Having a clear queue of priority pages is more useful than relying on guesswork or scattered edits  \n",
        "\n",
        "The long term aim is to raise the overall currency of information on Wikipedia pages about Zambia  \n",
        "Fresher content increases trust and value whether the pages are used in schools in media or in everyday research  \n",
        "When updates become routine and guided the quality of the whole collection improves steadily  \n",
        "\n",
        "These objectives align with the CRISP DM phase of Business Understanding  \n",
        "Here the aim is to define broad and specific goals while also staying close to the needs of the community that relies on and contributes to this knowledge  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSFSNZlUlEDF"
      },
      "source": [
        "This foundation ensures that later technical steps—like data preparation, modeling, and evaluation—stay tied to real impact and community relevance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNxJM4_U-TmF"
      },
      "source": [
        "## Data Mining\n",
        "\n",
        "To meet the business obejectives, our task is to develop a classification model that can label the recency of content on Zambian Wikipedia pages. Each section would be tagged as 'Current','Moderately Outdated' or 'Severely Outdated', giving contributors and readers a quick sense of accuracy. This turns the challenge of stale information into a clear, structured task.\n",
        "\n",
        "Beyond classification, we also aim to analyse edit histories to reveal how quickly different topics become outdated. Some pages, like those on politics, may need frequent updates, while others such as geography remain stable for longer. Spotting these trends helps contributors anticipate where attention will be needed next.\n",
        "\n",
        "Finally, the project should provide ranking metrics that highlight high-priority pages. Instead of random edits, contributors can follow a clear queue that shows where updates will have the most impact. In this way, data mining goals directly support fresher, more reliable information on Wikipedia pages about Zambia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEeGa11fRdIt"
      },
      "source": [
        "## Initial Project Success Criteria\n",
        "\n",
        "Our initial criteria for project success will be measured by:\n",
        "\n",
        "- **Classification Accuracy**  \n",
        "  The classification model should achieve an accuracy of at least 85% in correctly identifying the recency level of data on Zambian Wikipedia pages. This will serve as a key benchmark for determining whether the project is meeting its early objectives  \n",
        "\n",
        "- **Interpretability and Actionability**  \n",
        "  The predictions of the model and the factors influencing them must be understandable to human editors. Contributors should be able to see why a page is flagged as outdated and also get clear guidance on what needs updating. The results should be actionable so Wikipedians can focus their efforts where they matter most  \n",
        "\n",
        "- **Scalability**  \n",
        "  The model should be capable of being applied to a wide range of Zambian Wikipedia pages and not just a single Zambia page. Broader application will make the tool useful for long term and large scale improvements  \n",
        "\n",
        "- **Feedback Integration**  \n",
        "  Another measure of success will be how well the model's insights can be integrated into a tool or dashboard that Wikipedians can use. This would allow contributors to receive notifications or recommendations for content updates in a more structured and user friendly way  \n",
        "\n",
        "These criteria provide a practical way of measuring the relative success of the project and align with the \"Key Success Criteria\" aspect of the CRISP-DM Business Understanding phase  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ASf8SA-FO80"
      },
      "source": [
        "This section completes the first phase of the project under the CRISP-DM methodology which focuses on understanding the overall business context and clearly identifying the challenge that needs to be solved  \n",
        "\n",
        "The reliability and usefulness of Zambian Wikipedia pages as a source of current information is at risk when articles are left without updates for long periods of time  \n",
        "Information that may have been accurate at one point becomes outdated over time and this makes it harder for readers to trust that the content reflects the present situation in Zambia  \n",
        "\n",
        "There is currently no clear or systematic way to highlight which pages have stale or outdated data  \n",
        "Because of this contributors especially Zambian Wikipedians often do not know where to direct their efforts resulting in random or uneven updates where some articles receive frequent edits while others are overlooked  \n",
        "\n",
        "This lack of focus reduces the overall quality of information about Zambia on Wikipedia and makes the platform less useful for researchers students and the general public who rely on it for accurate and up to date content  \n",
        "\n",
        "The next phases of this project following the CRISP-DM process will move into Data Understanding and Data Preparation  \n",
        "For this checkpoint the deliverables include adding this content to the Google Colab notebook and creating a corresponding section in the README.md file with at least one commit per team member properly tagged to document individual contributions  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRWilUrxipW6"
      },
      "source": [
        "# 2. Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvpPVQhzk88t",
        "outputId": "e2af7d9a-3462-4d7e-efd9-be492d2eb295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# installing the necessay packages\n",
        "\n",
        "!pip install pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5sgnIw3AlegG",
        "outputId": "e0330907-e68b-4d8f-c25a-038a2e7427e4"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2641072912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# mount drive to colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VE7OauE7oSOQ"
      },
      "outputs": [],
      "source": [
        "# read the csv file containing the data set for our use case\n",
        "import pandas as pd\n",
        "\n",
        "recency_wiki_csv = pd.read_csv(\"/content/drive/MyDrive/CollabData/zambia_wikipedia_search.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rXYFnA8QqSx2"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fIPuCiZlqYbk"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NGYvh2ObqfT8"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t6Z7tNEfqkLx"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t29bzFk9qoYH"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv.head(1).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "efaeLJeZquRv"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv.tail().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8cVjWgDitP_x"
      },
      "outputs": [],
      "source": [
        " #  import matplotli.pyplot as plt for Data virsualization\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NXLbo6Fat6Oy"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv[['size', 'wordcount']].hist(bins=30, figsize=(10,5))\n",
        "plt.suptitle(\"Distribution of Article Size and Word Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FF1D-j6B04Yc"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "\n",
        "recency_wiki_csv['timestamp'] = pd.to_datetime(recency_wiki_csv['timestamp'], utc=True)\n",
        "recency_wiki_csv['days_since_last_edit'] = (datetime.now(timezone.utc) - recency_wiki_csv['timestamp']).dt.days\n",
        "recency_wiki_csv['days_since_last_edit'].hist(bins=30, figsize=(10,5))\n",
        "plt.title(\"Distribution of Days Since Last Edit\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Number of Pages\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AhTIvEWj0Jzm"
      },
      "outputs": [],
      "source": [
        "# Horizontal Barchart for top 10 Zambian Pages by word count\n",
        "\n",
        "top_pages = recency_wiki_csv.nlargest(10, 'wordcount')\n",
        "top_pages.plot(x='title', y='wordcount', kind='barh', figsize=(10,6))\n",
        "plt.title(\"Top 10 Zambia Pages by Word Count\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Page Title\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_PB6UGz8ji"
      },
      "source": [
        "# Summary of the observations seen\n",
        "\n",
        "###  1. Dataset Overview\n",
        "The dataset contains **500 Wikipedia pages** related to *“Zambia”*.\n",
        "\n",
        "**Columns include:**\n",
        "- `ns` (namespace)  \n",
        "- `title` (page title)  \n",
        "- `pageid` (unique page identifier)  \n",
        "- `size` (page size in bytes)  \n",
        "- `wordcount` (number of words on the page)  \n",
        "- `snippet` (short text excerpt from the page)  \n",
        "- `timestamp` (last edit timestamp)  \n",
        "\n",
        "\n",
        "###  2. Data Quality\n",
        "- **No null values** were detected in any of the columns.  \n",
        "- Column data types:  \n",
        "  - `ns`, `pageid`, `size`, `wordcount` → integers  \n",
        "  - `title`, `snippet`, `timestamp` → objects (strings)  \n",
        "- All **500 rows are complete**.  \n",
        "\n",
        "\n",
        "###  3. Numerical Attributes\n",
        "- **Page size (`size`)**: 244 bytes → 235,161 bytes  \n",
        "- **Word count (`wordcount`)**: 14 → 16,058 words  \n",
        "- Distributions are **skewed** with a few very large pages (outliers).  \n",
        "- Most pages are **moderate in size** (~4,000–18,000 bytes).  \n",
        "\n",
        "\n",
        "###  4. Timestamps\n",
        "- Latest edits in the dataset range from **2025-08-07 to 2025-08-13**.  \n",
        "- Converting the `timestamp` column to `datetime` allows calculation of **`days_since_last_edit`**.  \n",
        "\n",
        "\n",
        "###  5. Categorical Attributes\n",
        "- `title` is **unique** for each row.  \n",
        "- `ns` is **constant (0)** → all rows are main article namespace.  \n",
        "\n",
        "\n",
        "###  6. Observations\n",
        "- Dataset is **clean and complete**, with no missing values.  \n",
        "- `snippet` contains **HTML tags** (e.g., `<span class=\"searchmatch\">`) → can be cleaned for plain text analysis.  \n",
        "- Some pages are **much larger** (size & wordcount), showing a mix of **content-rich vs. stub articles**.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwvZLrYVhQi7"
      },
      "source": [
        "# DATA PREPARATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuLu7gDii4gl"
      },
      "source": [
        "####  Data preparation refers to preparing data to be used in the modeling phase. It includes data cleaning which involves addressing any data quality issues, feature engineerin which involves creatin new variables from existing ones that might be more useful for the model, data transformation which involves preparing the data for chosen algorithms like scaling numerical categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Tf278HnIZC"
      },
      "source": [
        "####  We begin by checking for null values or checking if our dataset contains any null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Wry_g7xkzVn"
      },
      "outputs": [],
      "source": [
        "recency_wiki_csv.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA6jVFMilxTy"
      },
      "source": [
        "As seen from the output above, there are no null or missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e163UqHEoU6f"
      },
      "source": [
        "#### We then select the columns that we are interested in or columns that are useful in our case, e.g the timestamp column will be used to see the actual recency of the post, size and the wordcount can be used to show relations like posts with larger sizes are most recently updated and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UQx4ifbTqa8F"
      },
      "outputs": [],
      "source": [
        "interested_in_columns = [\"title\", \"pageid\", \"size\", \"wordcount\", \"timestamp\"]\n",
        "\n",
        "# creating a dataframe containing the columns we need\n",
        "zambia_wiki_posts = recency_wiki_csv[interested_in_columns].copy()\n",
        "\n",
        "# to investigate and confirm the columns in the new dataframe by observing the first row\n",
        "zambia_wiki_posts.iloc[0].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYb5swIvvfci"
      },
      "source": [
        "#### We then rename the column names for some attributes to make them more descriptive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J4gsRL2av2oq"
      },
      "outputs": [],
      "source": [
        "zambia_wiki_posts = zambia_wiki_posts.rename(columns={\"size\": \"sizeInBytes\", \"timestamp\": \"lastEditTimeStamp\"})\n",
        "\n",
        "# confirming changes in names\n",
        "zambia_wiki_posts.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H5CexPLuChi"
      },
      "source": [
        "#### Now, we derive necessary attributes to be used in the modelling phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKd7zSYu5gP"
      },
      "source": [
        "These attributes will serve as key factors to help us in the modelling phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "__y668UUv2Ac"
      },
      "outputs": [],
      "source": [
        "# Converts the last edit timestamp to a datetime object and calculates how many days have passed since each post was last edited\n",
        "\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "zambia_wiki_posts[\"lastEditTimestampAsDatetime\"] = pd.to_datetime(zambia_wiki_posts[\"lastEditTimeStamp\"], utc=True)\n",
        "zambia_wiki_posts[\"daysSinceLastEdited\"] = (datetime.now(timezone.utc) - zambia_wiki_posts[\"lastEditTimestampAsDatetime\"]).dt.days\n",
        "\n",
        "# To observe and display the first row to see the changes made\n",
        "\n",
        "zambia_wiki_posts.iloc[0].T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rgrTYNW_xs55"
      },
      "outputs": [],
      "source": [
        "# To observe and display the 6th row to see the changes made\n",
        "\n",
        "zambia_wiki_posts.iloc[5].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8gd4vl73mcB"
      },
      "source": [
        "#### Given that we are dealing with numbers in size, wordcount and days since last edited, there is need to scale the numbers down so that all the values will be in the range of 0 and 1 using the MinMaxScaler. Scaling the numbers to some range like 0 to 1 help in ensuring fairness so that column values like sizeInBytes do not dominate the column values like for daysSinceLastEdit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aYIRrJiP4Q3f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "columns_to_scale = [\"sizeInBytes\", \"wordcount\", \"daysSinceLastEdited\"]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# keeping the old original attributes and the new ones appending the new ones with Scaled after the actual field name\n",
        "zambia_wiki_posts[[column + \"Scaled\" for column in columns_to_scale]] = scaler.fit_transform(\n",
        "    zambia_wiki_posts[columns_to_scale]\n",
        ")\n",
        "\n",
        "# observing the output of the first 2 rows to see if changes have taken effect\n",
        "zambia_wiki_posts.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX1KbxF3C6sE"
      },
      "source": [
        "# 4. Modeling\n",
        "\n",
        "In this phase, we aim to build and train one or more data mining models to classify Zambian Wikipedia pages based on how recently they were edited.  \n",
        "The goal is to assign pages to one of three categories:  \n",
        "- Current  \n",
        "- Moderately Outdated  \n",
        "- Severely Outdated\n",
        "\n",
        "This follows the CRISP-DM modeling step, where we select algorithms, split the data, train models, and observe preliminary patterns before formal evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2rmJDuETDLcR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# We use this function to add the recency label\n",
        "def fxn_recency_category(days):\n",
        "    if days <= 30:\n",
        "        return \"Current\"\n",
        "    elif days <= 180:\n",
        "        return \"Moderately Outdated\"\n",
        "    else:\n",
        "        return \"Severely Outdated\"\n",
        "\n",
        "zambia_wiki_posts[\"recency_label\"] = zambia_wiki_posts[\"daysSinceLastEdited\"].apply(fxn_recency_category)\n",
        "\n",
        "# Saving new CSV file for modeling\n",
        "zambia_wiki_posts.to_csv(\"./zambia_wiki_prepared_for_modeling.csv\", index=False)\n",
        "print(\"Prepared CSV saved: 'zambia_wiki_prepared_for_modeling.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWav1-duNW-O"
      },
      "source": [
        "**Features (X) and Target (y):**\n",
        "\n",
        "- **Features (X):**  \n",
        "    - `sizeInBytesScaled`, `wordcountScaled`, `daysSinceLastEditedScaled`  \n",
        "    These scaled numeric columns represent page size, content depth, and time since last edit. They are directly relevant to predicting recency.\n",
        "\n",
        "- **Target (y):**  \n",
        "    - `recency_label`  \n",
        "    This is the categorical label we aim to predict: Current, Moderately Outdated, or Severely Outdated.\n",
        "\n",
        "**Why these columns:**  \n",
        "- Days since last edit is the most informative feature for recency.  \n",
        "- Size and word count help capture differences between pages that are large but may not have been updated recently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FLWziRsBjkz4"
      },
      "outputs": [],
      "source": [
        "# Defining features and target\n",
        "feature_columns = [\"sizeInBytesScaled\", \"wordcountScaled\", \"daysSinceLastEditedScaled\"]\n",
        "target_column = \"recency_label\"\n",
        "\n",
        "X = zambia_wiki_posts[feature_columns]\n",
        "y = zambia_wiki_posts[target_column]\n",
        "\n",
        "# Splitting the dataset (80% train / 20% test) with statification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X6PRA3eYyCkk"
      },
      "outputs": [],
      "source": [
        "# Train Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "print(\"Decision Tree training complete.\")\n",
        "\n",
        "# Train Random Forest\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Random Forest training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fDqiaZNRGli"
      },
      "source": [
        "\\**Preliminary Observations:**\n",
        "\n",
        "- Both models were able to learn the patterns in the dataset quickly.  \n",
        "- The `daysSinceLastEditedScaled` feature appears to dominate the classification, which aligns with our intuition: pages that were edited recently are more likely to be labeled \"Current.\"  \n",
        "- The Decision Tree is fully interpretable and shows how thresholds on days since last edit and page size help classify recency.  \n",
        "- Random Forest builds on this by combining multiple trees, providing slightly more robustness even if the dataset were noisier.  \n",
        "\n",
        "These observations are part of the modeling phase-they give a first glance at how well our features can inform recency classification before formal evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzjlswKXGn2Y"
      },
      "source": [
        "# 5. Evaluation\n",
        "\n",
        "After training both the Decision Tree and the Random Forest models, we tested them on the 20% hold-out test set (100 pages).  \n",
        "The idea was to see how well they can correctly classify pages into **Current**, **Moderately Outdated**, and **Severely Outdated**.\n",
        "\n",
        "\n",
        "\n",
        "### Decision Tree Results\n",
        "\n",
        "- **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KlKNjmN3G9o4"
      },
      "outputs": [],
      "source": [
        "y_pred_dt = dt_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred_dt))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0wdH8-bHEsE"
      },
      "source": [
        "### Random Forest Results\n",
        "\n",
        "- **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9HPPAXRXHJlh"
      },
      "outputs": [],
      "source": [
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Accuracy:** 0.99 (99%)\n",
        "- Only one page was misclassified in the **Severely Outdated** class, but overall performance was still very high.\n",
        "\n",
        "\n",
        "\n",
        "### Observations\n",
        "\n",
        "- Both models performed extremely well, which suggests that the features we used (`sizeInBytes`, `wordcount`, and `daysSinceLastEdited`) are very strong indicators of recency.\n",
        "- The **Decision Tree** hit a perfect score, but that might also mean it could be overfitting slightly.\n",
        "- The **Random Forest** gave almost the same results, but is more robust since it averages across many trees.\n",
        "- Either model would work, but for deployment we chose the **Random Forest** because it’s usually more stable in practice."
      ],
      "metadata": {
        "id": "hlp7ReT5Pkc_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4H5CexPLuChi"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}